# 大语言模型基础

语言模型（LM）的根本任务是计算一个词序列（即一个句子）出现的概率

## 语言模型与transformer架构

### 发展历程

1. 统计语言模型与N-GRAM思想
2. 神经网络语言模型与词嵌入
3. 循环神经网络(RNN)与长时记忆网络(LSTM)

### transformer架构解析

1. Encoder-Decoder整体结构:编码器的任务是"理解"输入的整个句子,解码器的任务是"生成"目标句子
2. 从自注意力到多头注意力:query,key,value
3. 前馈神经网络
4. 残差链接与层归一化

### Decoder-only架构

语言的核心任务,是预测下一个最有可能出现的词

工作模式:自回归(掩码自注意力机制)

优势:

- 训练目标统一
- 结构简单,易于扩展
- 天然适合生成任务

## 与大语言模型交互

### 提示词工程

模型采样参数:temperature,Top-k,Top-p

零样本,单样本,少样本提示

指令调优:极大地简化了我们与模型交互的方式,使得直接,清晰的自然语言指令成为可能

基础提示技巧:角色扮演,上下文示例

思维链(COT):添加一句引导语,如"请逐步思考"或"Let's think step by step"

### 文本分词

分词（tokenization）：计算机本质上只能理解数字，所以在将自然语言喂给大语言模型之前，必须先将其转换成模型能够处理的数字格式。这个将文本序列转化成数字序列的过程，称为分词

分词器：定义一套规则，将原始文本切分成一个个最小的单元，即**词元**（Token）

分词器对开发者的意义：

- 上下文窗口限制：精确管理输入长度，避免超出上下文限制
- API成本：节省token，预估和控制智能体运行成本
- 模型表现的异常：优势模型的奇怪表现根源在于分词

### 调用开源大语言模型

### 模型的选择

关键考量因素：性能与能力，成本，速度（延迟），上下文窗口，部署方式，生态与工具链，可微调与定制化，安全性与伦理

## 缩放法则与局限性

### 模型幻觉

根据幻觉的表现形式可以分为多个种类，如：

- 事实性幻觉：生成与现实世界事实不符的信息
- 忠实性幻觉：在文本摘要、翻译等任务中，生成的内容未能忠实地反应源文本的含义
- 内在幻觉：模型生成的内容与输入信息相矛盾

幻觉的产生是多方面的因素：

- 首先，训练数据中可能包含错误或矛盾的信息。

- 其次，模型的自回归生成机制决定了它只是在预测下一个最可能的词元，而没有内置的事实核查模块。
- 最后，在面对需要复杂推理的任务时，模型可能会在逻辑链条中出错，从而“编造”出错误的结论

大语言模型还面临知识时效性不足和训练数据中存在的偏见等挑战。大语言模型的能力来源于其训练数据。这意味着模型所掌握的知识是其训练数据收集时的最新材料。对于在此日期之后发生的事情、新出现的概念或最新的事实，模型将无法感知或正确回答。于此同时训练数据往往包含了人类社会的各种偏见和刻板印象。

检测和缓解幻觉的方法：

- 数据层面：通过高质量数据清洗、引入事实性知识以及强化学习和人类反馈（RLHF）等方式，从源头减少幻觉
- 模型层面：探索新的模型架构，或让模型能够表达其对生成内容的不确定性
- 推理与生成层面：
  1. 检索增强生成（RAG）：这是目前缓解幻觉的有效方法之一。RAG系统通过在生成之前从外部知识库（如文档数据库、网页）中检索相关信息，然后将检索到的信息作为上下文，引导模型生成基于事实的回答
  2. 多步推理与验证：引导模型进行多步推理，并在每一步进行自我检查或外部验证
  3. 引入外部工具：允许模型调用外部工具（如搜索引擎、计算器、代码解释器）来获取实时信息或进行精确计算
